{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Neural style transfer Projeto da mat\u00e9ria de Redes Neurais do curso de Doutorado em Modelagem Matem\u00e1tica da FGV-EMAp lecionada pelo professor Renato Rocha Souza. Nesse trabalho, reproduzo o artigo de transfer\u00eancia de estilo A Neural Algorithm of Artistic Style, de Leon A. Gays, Alexander S. Ecker e Matthis Bethge. O trabalho \u00e9 composto pela implementa\u00e7\u00e3o do algoritmo com o Pytorch e sua utiliza\u00e7\u00e3o para a transfer\u00eancia de estilo de um v\u00eddeo. Al\u00e9m disso, \u00e9 proposto uma altera\u00e7\u00e3o para a transfer\u00eancia de estilo para v\u00eddeos.","title":"Home"},{"location":"#neural-style-transfer","text":"Projeto da mat\u00e9ria de Redes Neurais do curso de Doutorado em Modelagem Matem\u00e1tica da FGV-EMAp lecionada pelo professor Renato Rocha Souza. Nesse trabalho, reproduzo o artigo de transfer\u00eancia de estilo A Neural Algorithm of Artistic Style, de Leon A. Gays, Alexander S. Ecker e Matthis Bethge. O trabalho \u00e9 composto pela implementa\u00e7\u00e3o do algoritmo com o Pytorch e sua utiliza\u00e7\u00e3o para a transfer\u00eancia de estilo de um v\u00eddeo. Al\u00e9m disso, \u00e9 proposto uma altera\u00e7\u00e3o para a transfer\u00eancia de estilo para v\u00eddeos.","title":"Neural style transfer"},{"location":"neural_style_transfer/","text":"Neural Style Transfer Reprodu\u00e7\u00e3o do algoritmo de transfer\u00eancia de estilo do artigo A Neural Algorithm of Artistic Style , de Leon A. Gays, Alexander S. Ecker e Matthis Bethge. import torch import urllib import torch.nn as nn from PIL import Image import pickle as pkl from torchvision import transforms import torch.optim as optim from torchvision.models import vgg19 from tqdm.notebook import tqdm import matplotlib.pyplot as plt %matplotlib inline Introdu\u00e7\u00e3o Em arte, principalmente em pinturas, os humanos dominaram a constru\u00e7\u00e3o de experi\u00eancias visuais que s\u00e3o formadas por uma complexa combina\u00e7\u00e3o do conte\u00fado e do estilo. Tal produ\u00e7\u00e3o ainda n\u00e3o possui um algoritmo, e se \u00e9 questionado se \u00e9 poss\u00edvel a exist\u00eancia de um algoritmo de cria\u00e7\u00e3o de grandes obras de arte. A \u00e1rea de visualiza\u00e7\u00e3o teve um grande desenvolvimento com o uso de redes neurais e Deep Learning, as redes convolucionais s\u00e3o extremamente potentes, sendo capazes de detectar objetos em imagens, detectar padr\u00f5es. Quando as CNN s\u00e3o treinadas para a detec\u00e7\u00e3o de objetos, as camadas da rede aprendem a construir uma representa\u00e7\u00e3o do conte\u00fado da imagem que vai al\u00e9m dos simples valores dos pixels, essa representa\u00e7\u00e3o cont\u00e9m as principais caracter\u00edsticas necess\u00e1rias para as \u00faltimas camadas detectar qual \u00e9 o objeto. Nesse trabalho, n\u00f3s usaremos o resultado das \u00faltimas camadas de uma rede convolucional como a representa\u00e7\u00e3o do conte\u00fado (content representation) da imagem de entrada. Para a representa\u00e7\u00e3o do estilo (style representation) n\u00f3s vamos utilizar um filtro que captura texturas, mas usaremos esse em diferentes camadas da rede para obter uma representa\u00e7\u00e3o global do estilo. A principal descoberta desse artigo foi que a representa\u00e7\u00e3o do conte\u00fado e do estilo de uma imagem s\u00e3o separ\u00e1veis a partir dos resultados da rede convolucional. Como podemos obter a representa\u00e7\u00e3o de estilo de uma imagem e a representa\u00e7\u00e3o de conte\u00fado de uma imagem, o m\u00e9todo proposto para trasfer\u00eancia de estilo faz \u00e9 utilizar duas imagens, a imagem de conte\u00fado e a imagem de estilo, obter a representa\u00e7\u00e3o delas, e com uma nova imagem, que pode ser um ru\u00eddo qualquer, otimizar essa imagem para diminuir a dist\u00e2ncia da sua representa\u00e7\u00e3o de estilo com a representa\u00e7\u00e3o de estilo da imagem de estilo e diminuir a dist\u00e2ncia da sua representa\u00e7\u00e3o de conte\u00fado com a representa\u00e7\u00e3o de conte\u00fado da imagem de conte\u00fado. Pr\u00e9-processamento das imagens Usaremos a rede pr\u00e9-treinada VGG-19 como rede convolucional para a representa\u00e7\u00e3o de conte\u00fado e estilo de imagens. A rede foi apresentada em 2014, e a vers\u00e3o VGG-19 em espec\u00edfico possui 19 camadas de profundidade. Como n\u00f3s vamos utilizar a rede pr\u00e9 treinada VGG-19, devemos adequar as nossas imagens para essa rede. As entradas devem ter dimens\u00e3o \\([224, 224]\\) . Para isso, iremos transformar a iamgem original para ter largura \\(256\\) e pegar o quadrado \\([224, 224]\\) central. Al\u00e9m disso, o ideal para a rede \u00e9 uma imagem com m\u00e9dia \\([0.485, 0.456, 0.406]\\) e desvio padr\u00e3o \\([0.229, 0.224, 0.225]\\) . Essas condi\u00e7\u00f5es de entrada da rede VGG-19 se tornam um grande limite para o nosso m\u00e9todo, pois n\u00e3o podemos treinar imagens em alta resolu\u00e7\u00e3o com grandes dimens\u00f5es. Al\u00e9m disso, existe uma pequena dificuldade em retornar a imagem para a m\u00e9dia e o desvio padr\u00e3o originais. preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) def load_image(path): '''Function that load the image and preprocess to the VGG19 format The image must be with shape 224x224, with mean [0.485, 0.456, 0.406] and standard deviation [0.229, 0.224, 0.225]''' img = Image.open(path) return preprocess(img).unsqueeze(0) Modelo O modelo ir\u00e1 utilizar a rede convolucional VGG_19 que a API do Pytorch possibilita o acesso. Ele ir\u00e1 receber duas lista de \u00edndices de quais camadas deve utilizar para obter a representa\u00e7\u00e3o de conte\u00fado e de estilo, por padr\u00e3o, as camadas utilizadas no paper s\u00e3o as seguintes: Conte\u00fado : [21] = ['Conv4_2'] Estilo : [0, 5, 10, 19, 28] = ['Conv1_1', 'Conv2_1', 'Conv3_1', 'Conv4_1', 'Conv5_1'] O treinamento neste trabalho utiliza as mesmas camadas, mas este \u00e9 um par\u00e2metro ajust\u00e1vel da fun\u00e7\u00e3o. O que o modelo faz \u00e9 receber a imagem de treinamento, passar ela pelas camadas e verificar se a camada \u00e9 ou uma camada de estilo ou de conte\u00fado, e caso seja, ele salva esse resultado. class vgg19_mod(nn.Module): def __init__(self, content_layers, style_layers): ''' Class that initialize the VGG19 model and set the intermediate style and content layers Inputs: style_layers : list of index of the style layers in the VGG19 net content_layers: list of index of the content layers in the VGG19 net ''' super(vgg19_mod, self).__init__() features = list(vgg19(pretrained = True).features) self.features = nn.ModuleList(features).eval() self.style_layers = style_layers self.content_layers = content_layers def forward(self, input): ''' Forward process that pass the input image trought the VGG 19 net and keeps the intermediates outputs Inputs: input : tensor of shape [batch, 3, 224, 224] Outputs: style_out : list of style layers outputs cotent_out : list of content layers outputs ''' style_out = [] content_out = [] for i, model in enumerate(self.features): input = model(input) if i in self.style_layers: style_out.append(input) if i in self.content_layers: content_out.append(input) return content_out, style_out Fun\u00e7\u00e3o de perda Seja \\(\\pmb x\\) o tensor da imagem que vamos treinar, \\(\\pmb p\\) \u00e9 o tensor da imagem de conte\u00fado e \\(\\pmb a\\) o tensor da imagem de estilo. A fun\u00e7\u00e3o de perda (loss) ser\u00e1 definida por dois termos, a perda de conte\u00fado e de estilo. Definimos \\(F^l\\) como a representa\u00e7\u00e3o de conte\u00fado de \\(\\pmb x\\) na camada \\(l\\) e \\(P^l\\) a representa\u00e7\u00e3o de conte\u00fado de \\(\\pmb p\\) na camada \\(l\\) . Se \\(l\\) \u00e9 uma camada que gera \\(N_l\\) filtros com um total de \\(M_l\\) elementos (largura vezes altura), ent\u00e3o \\(F_l \\in \\mathbb {R}^{N_l \\times M_l}\\) e \\(P_l \\in \\mathbb {R}^{N_l \\times M_l}\\) . A nossa perda ser\u00e1 a dist\u00e2ncia quadr\u00e1tica m\u00e9dia \\(F_l\\) e \\(P_l\\) . \\(L_{content}(\\pmb x, \\pmb p, \\pmb l) = \\dfrac{1}{N_l M_l}\\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2\\) def content_loss(pred, target): '''The content loss is the mean squared error''' return torch.pow(pred - target, 2).mean() Para a perda do estilo, em cada uma das camadas usadas para o estilo, iremos utilizar um m\u00e9todo para c\u00e1lculo de textura que computa a correla\u00e7\u00e3o, esse fator \u00e9 a matriz de Gram \\(G_l\\) , que \u00e9 definida como, o produto interno das colunas de \\(F_l\\) (a representa\u00e7\u00e3o do tensor que estamos treinando \\(\\pmb x\\) ). Agora seja \\(\\pmb a\\) o tensor de estilo e \\(A_l\\) a matriz de Gram calculada para a representa\u00e7\u00e3o de \\(\\pmb a\\) na camda \\(l\\) . A perda do estilo ser\u00e1 definida como: \\(L_{style}(\\pmb x, \\pmb a, \\pmb l) = \\dfrac{1}{N_l M_l}\\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2\\) def gram_matrice(input): ''' Compute the gram matrice of the input tensor The gram matrice of V is V'V, but we have to change the input to 2 dimensios Than we normalize by the number of elements ''' batch, channel, width, height = input.size() M = input.view(batch * channel, width * height) gram = torch.mm(M, M.t()) return gram.div(batch*width*height*channel) def style_loss(pred, gram_target): '''The style loss if the euclidian distance of the Gram matrice ''' gram_pred = gram_matrice(pred) return torch.pow(gram_pred - gram_target, 2).mean() A perda final tem ent\u00e3o a seguinte forma: $L_{total}(\\pmb x, \\pmb p, \\pmb a) = \\alpha L_{content}(\\pmb x, \\pmb p) + \\beta L_{style}(\\pmb x, \\pmb a) $ Sendo \\(\\alpha\\) e \\(\\beta\\) os par\u00e2metros que escolhemos para o treinamento dar mais prioridade para o conte\u00fado e para o estilo, reespectivamente. Treinamento Por fim, o treinamento ocorre da seguinte forma: 1. Instanciamos o modelo. 2. Obtemos as representa\u00e7\u00f5es de conte\u00fado e de estilo das imagens de conte\u00fado e de estilo, reespectivamente. 3. Definimos uma imagem que ser\u00e1 otimizada, o valor inicial dela \u00e9 o mesmo da imagem de conte\u00fado. 4. Instanciamos o otimizador L-BFGS. 5. Para cada itera\u00e7\u00e3o da otimiza\u00e7\u00e3o: * Computamos a representa\u00e7\u00e3o de conte\u00fado e de estilo da imagem de treinamento. * Computamos a perda de conte\u00fado e de estilo das representa\u00e7\u00f5es. * Com o m\u00e9todo de backward do Pytorch, computamos os gradientes da perda. * Passamos o valor da perda e o valor do gradiente para o m\u00e9todo do L-BFGS dar um passo (step). 6. Por fim, invertemos a transforma\u00e7\u00e3o inicial de pr\u00e9-processamento, retornando a m\u00e9dia e o desvio padr\u00e3o originais. def run_style_transfer(content_path, style_path, iterations = 20, content_layers = None, style_layers = None, content_weight = 1, style_weight = 1000000, verbose = False, cuda = False, return_loss = False): ''' Training process for the neural style transfer VGG19 The method uses two images, the content and the style image, and generate a image that contain the content and the style of the reespective images The optimization is done by LBFGS Inputs: content_path : path for the content image style_path : path for the style image iterations: number of iterations to converge content_layers : list of index of VGG19 content layers style_layers : list of index of VGG19 style layers content_weight : weight of the content loss style_weight : weight of the style loss verbose : if the function will print intermediate steps cuda: if the function will process using GPU return_loss: if the function will return the loss value for each epoch Outputs: init_image : result image with content and style ''' if cuda: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if verbose: print(\"Running model with\", device) else: device = torch.device('cpu') #Create loss dict if return_loss: losses = {'style_loss': [], 'content_loss': [], 'total_loss':[]} #Setting intermediate layers if content_layers is None: content_layers = [22] if style_layers is None: style_layers = [1, 6, 11, 20, 29] #Loading the model model = vgg19_mod(content_layers, style_layers).to(device) if verbose: print('Initialized model.') #Loading the images as tensors content_img = load_image(content_path).to(device) style_img = load_image(style_path).to(device) init_img = load_image(content_path).to(device) if verbose: print('Initialized images.') #Getting the content and style outputs content_out, _ = model(content_img) _, style_out = model(style_img) if verbose: print('Calculated images outputs.') #Pre-calculating the gram matrice for the styles outputs gram_out = [gram_matrice(out) for out in style_out] #Setting optmizer optmizer = optim.LBFGS([init_img.requires_grad_()]) if verbose: print('Starting optimization.') for iter in tqdm(range(iterations)): if return_loss: _iter_style_loss = [] _iter_content_loss = [] _iter_total_loss = [] def closure(): '''Calculate ouputs, loss and gradients''' optmizer.zero_grad() init_content_out, init_style_out = model(init_img) _content_loss = 0 for l in range(len(content_out)): _content_loss += content_loss(init_content_out[l], content_out[l]) _style_loss = 0 for l in range(len(style_out)): _style_loss += style_loss(init_style_out[l], gram_out[l]) _content_loss *= content_weight _style_loss *= style_weight loss = _content_loss + _style_loss loss.backward(retain_graph=True) if iter % 100 == 0 and iter > 0: if verbose: print('Iteration %d. Model loss %.8f'%(iter, loss)) print('Content loss: %.8f | Style loss: %.8f'%(_content_loss, _style_loss)) print() #Update loss dict if return_loss: _iter_style_loss.append(_style_loss.clone().data.cpu().numpy()) _iter_content_loss.append(_content_loss.clone().data.cpu().numpy()) _iter_total_loss.append(loss.clone().data.cpu().numpy()) return _content_loss + _style_loss optmizer.step(closure) if return_loss: losses['style_loss'].append(_iter_style_loss[-1]) losses['content_loss'].append(_iter_content_loss[-1]) losses['total_loss'].append(_iter_total_loss[-1]) #Invert VGG19 input transformation invTrans = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]), transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])]) inv_img = invTrans(init_img.squeeze(0)) if return_loss: return inv_img, losses else: return inv_img Exemplos Gata e Noite estrelada de Van Gogh Como primeiro exemplo, vamos utilizar a imagem de uma gata e a pintura Noite Estrelada do Van Gogh, no entanto, vamos variar os par\u00e2metros \\(\\alpha\\) e \\(\\beta\\) para observar o resultado na imagem final. Na realidade, iremos variar apenas a ra\u00e7\u00e3o \\(\\dfrac{\\beta}{\\alpha}\\) , pois o resultado \u00e9 o mesmo. cat_path = '/content/cat.jpg' stary_night_path = '/content/stary_night.jpg' cat_stary_dict = {} for ratio in [1e5, 1e6, 1e7]: cat_stary_dict[ratio] = run_style_transfer(cat_path, stary_night_path, verbose = False, cuda = True, content_weight = 1, style_weight = ratio) A imagem utilizada como conte\u00fado e como estilo s\u00e3o as seguintes. fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Content image') content_image = Image.open('cat.jpg') ax[0].imshow(content_image) ax[0].axis('off') ax[1].set_title(\"Style image\") style_image = Image.open('stary_night.jpg') ax[1].imshow(style_image) ax[1].axis('off') plt.show() E o resultado final \u00e9 o seguinte, para cada uma das raz\u00f5es: fig, ax = plt.subplots(nrows = 1, ncols = 3) fig.set_figwidth(20) fig.set_figheight(12) for i, ratio in enumerate([1e5, 1e6, 1e7]): img = cat_stary_dict[ratio].cpu().detach().permute(1, 2, 0).data.clamp_(0, 1).numpy() #img -= img.min() #img /= img.max() #print(img.min(), img.max()) ax[i].imshow(img) ax[i].set_title(\"Style/content weight ratio:\" + str(ratio)) ax[i].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show() Cachorro e pintura O Grito de Edvard Munch dog_path = '/content/little_dog.jpg' grito_path = '/content/scream.jpg' dog_grito_dict = {} for ratio in [1e5, 1e6, 1e7]: dog_grito_dict[ratio] = run_style_transfer(dog_path, grito_path, iterations = 50, verbose = False, cuda = True, content_weight = 1, style_weight = ratio) fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Content image') content_image = Image.open('little_dog.jpg') ax[0].imshow(content_image) ax[0].axis('off') ax[1].set_title(\"Style image\") style_image = Image.open('scream.jpg') ax[1].imshow(style_image) ax[1].axis('off') plt.show() fig, ax = plt.subplots(nrows = 1, ncols = 3) fig.set_figwidth(20) fig.set_figheight(12) for i, ratio in enumerate([1e5, 1e6, 1e7]): img = dog_grito_dict[ratio].cpu().detach().permute(1, 2, 0).data.clamp_(0, 1).numpy() ax[i].imshow(img) ax[i].set_title(\"Style/content weight ratio:\" + str(ratio)) ax[i].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show() An\u00e1lise do treinamento Vamos come\u00e7ar observando o comportamento da loss ao decorrer das itera\u00e7\u00f5es e encontrar o n\u00famero de itera\u00e7\u00f5es que otimiza o tempo de processamento e o valor da loss. Queremos obter esse valor \u00f3timo para sermos capazes de processar uma grande quantidade de imagens, como \u00e9 necess\u00e1rio ao aplicarmos o m\u00e9todo em um v\u00eddeo. cat_stary, cat_stary_loss = run_style_transfer('cat.jpg', 'stary_night.jpg', iterations = 100, cuda = True, return_loss = True) fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Model losses') ax[0].plot(list(range(len(cat_stary_loss['content_loss']))), cat_stary_loss['content_loss'], linewidth = 3, label = 'Content loss') ax[0].plot(list(range(len(cat_stary_loss['style_loss']))), cat_stary_loss['style_loss'], linewidth = 3, label = 'Style loss') ax[0].plot(list(range(len(cat_stary_loss['total_loss']))), cat_stary_loss['total_loss'], linewidth = 3, label = 'Total loss') ax[0].legend() ax[1].set_title(\"Style loss\") ax[1].set_title('Model losses') ax[1].plot(list(range(len(cat_stary_loss['content_loss']))), cat_stary_loss['content_loss'], linewidth = 3, label = 'Content loss') ax[1].plot(list(range(len(cat_stary_loss['style_loss']))), cat_stary_loss['style_loss'], linewidth = 3, label = 'Style loss') ax[1].plot(list(range(len(cat_stary_loss['total_loss']))), cat_stary_loss['total_loss'], linewidth = 3, label = 'Total loss') ax[1].set_ylim(0, 10) ax[1].set_xlim(0, 100) ax[1].legend() plt.show() Vemos que o m\u00e9todo tem uma r\u00e1pida converg\u00eancia, mantendo o mesmo valor da loss para todas as itera\u00e7\u00f5es ap\u00f3s as 30 primeiras. Dessa forma, podemos realizar o treinamento com apenas 30 itera\u00e7\u00f5es para evitar extra processamento. images = [('/content/jovem.jpg','/content/natureza_morta.jpg'), ('/content/fgv.jpg', '/content/picasso.jpg')] images_style = [] for im in images: images_style.append(run_style_transfer(im[0], im[1], iterations = 20, verbose = False, cuda = True, style_weight = ratio)) fig, ax = plt.subplots(nrows = 2, ncols = 2) ax = ax.flatten() fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Content image') content_image = Image.open(images[0][0]) ax[0].imshow(content_image) ax[0].axis('off') ax[1].set_title(\"Style image\") style_image = Image.open(images[0][1]) ax[1].imshow(style_image) ax[1].axis('off') ax[2].set_title('Content image') content_image = Image.open(images[1][0]) ax[2].imshow(content_image) ax[2].axis('off') ax[3].set_title(\"Style image\") style_image = Image.open(images[1][1]) ax[3].imshow(style_image) ax[3].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show() fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(12) for i, im in enumerate(images_style): img_to_plot = im.cpu().detach().permute(1, 2, 0).data.clamp_(0, 1).numpy() ax[i].imshow(img_to_plot) ax[i].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show()","title":"Implementa\u00e7\u00e3o"},{"location":"neural_style_transfer/#neural-style-transfer","text":"Reprodu\u00e7\u00e3o do algoritmo de transfer\u00eancia de estilo do artigo A Neural Algorithm of Artistic Style , de Leon A. Gays, Alexander S. Ecker e Matthis Bethge. import torch import urllib import torch.nn as nn from PIL import Image import pickle as pkl from torchvision import transforms import torch.optim as optim from torchvision.models import vgg19 from tqdm.notebook import tqdm import matplotlib.pyplot as plt %matplotlib inline","title":"Neural Style Transfer"},{"location":"neural_style_transfer/#introducao","text":"Em arte, principalmente em pinturas, os humanos dominaram a constru\u00e7\u00e3o de experi\u00eancias visuais que s\u00e3o formadas por uma complexa combina\u00e7\u00e3o do conte\u00fado e do estilo. Tal produ\u00e7\u00e3o ainda n\u00e3o possui um algoritmo, e se \u00e9 questionado se \u00e9 poss\u00edvel a exist\u00eancia de um algoritmo de cria\u00e7\u00e3o de grandes obras de arte. A \u00e1rea de visualiza\u00e7\u00e3o teve um grande desenvolvimento com o uso de redes neurais e Deep Learning, as redes convolucionais s\u00e3o extremamente potentes, sendo capazes de detectar objetos em imagens, detectar padr\u00f5es. Quando as CNN s\u00e3o treinadas para a detec\u00e7\u00e3o de objetos, as camadas da rede aprendem a construir uma representa\u00e7\u00e3o do conte\u00fado da imagem que vai al\u00e9m dos simples valores dos pixels, essa representa\u00e7\u00e3o cont\u00e9m as principais caracter\u00edsticas necess\u00e1rias para as \u00faltimas camadas detectar qual \u00e9 o objeto. Nesse trabalho, n\u00f3s usaremos o resultado das \u00faltimas camadas de uma rede convolucional como a representa\u00e7\u00e3o do conte\u00fado (content representation) da imagem de entrada. Para a representa\u00e7\u00e3o do estilo (style representation) n\u00f3s vamos utilizar um filtro que captura texturas, mas usaremos esse em diferentes camadas da rede para obter uma representa\u00e7\u00e3o global do estilo. A principal descoberta desse artigo foi que a representa\u00e7\u00e3o do conte\u00fado e do estilo de uma imagem s\u00e3o separ\u00e1veis a partir dos resultados da rede convolucional. Como podemos obter a representa\u00e7\u00e3o de estilo de uma imagem e a representa\u00e7\u00e3o de conte\u00fado de uma imagem, o m\u00e9todo proposto para trasfer\u00eancia de estilo faz \u00e9 utilizar duas imagens, a imagem de conte\u00fado e a imagem de estilo, obter a representa\u00e7\u00e3o delas, e com uma nova imagem, que pode ser um ru\u00eddo qualquer, otimizar essa imagem para diminuir a dist\u00e2ncia da sua representa\u00e7\u00e3o de estilo com a representa\u00e7\u00e3o de estilo da imagem de estilo e diminuir a dist\u00e2ncia da sua representa\u00e7\u00e3o de conte\u00fado com a representa\u00e7\u00e3o de conte\u00fado da imagem de conte\u00fado.","title":"Introdu\u00e7\u00e3o"},{"location":"neural_style_transfer/#pre-processamento-das-imagens","text":"Usaremos a rede pr\u00e9-treinada VGG-19 como rede convolucional para a representa\u00e7\u00e3o de conte\u00fado e estilo de imagens. A rede foi apresentada em 2014, e a vers\u00e3o VGG-19 em espec\u00edfico possui 19 camadas de profundidade. Como n\u00f3s vamos utilizar a rede pr\u00e9 treinada VGG-19, devemos adequar as nossas imagens para essa rede. As entradas devem ter dimens\u00e3o \\([224, 224]\\) . Para isso, iremos transformar a iamgem original para ter largura \\(256\\) e pegar o quadrado \\([224, 224]\\) central. Al\u00e9m disso, o ideal para a rede \u00e9 uma imagem com m\u00e9dia \\([0.485, 0.456, 0.406]\\) e desvio padr\u00e3o \\([0.229, 0.224, 0.225]\\) . Essas condi\u00e7\u00f5es de entrada da rede VGG-19 se tornam um grande limite para o nosso m\u00e9todo, pois n\u00e3o podemos treinar imagens em alta resolu\u00e7\u00e3o com grandes dimens\u00f5es. Al\u00e9m disso, existe uma pequena dificuldade em retornar a imagem para a m\u00e9dia e o desvio padr\u00e3o originais. preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) def load_image(path): '''Function that load the image and preprocess to the VGG19 format The image must be with shape 224x224, with mean [0.485, 0.456, 0.406] and standard deviation [0.229, 0.224, 0.225]''' img = Image.open(path) return preprocess(img).unsqueeze(0)","title":"Pr\u00e9-processamento das imagens"},{"location":"neural_style_transfer/#modelo","text":"O modelo ir\u00e1 utilizar a rede convolucional VGG_19 que a API do Pytorch possibilita o acesso. Ele ir\u00e1 receber duas lista de \u00edndices de quais camadas deve utilizar para obter a representa\u00e7\u00e3o de conte\u00fado e de estilo, por padr\u00e3o, as camadas utilizadas no paper s\u00e3o as seguintes: Conte\u00fado : [21] = ['Conv4_2'] Estilo : [0, 5, 10, 19, 28] = ['Conv1_1', 'Conv2_1', 'Conv3_1', 'Conv4_1', 'Conv5_1'] O treinamento neste trabalho utiliza as mesmas camadas, mas este \u00e9 um par\u00e2metro ajust\u00e1vel da fun\u00e7\u00e3o. O que o modelo faz \u00e9 receber a imagem de treinamento, passar ela pelas camadas e verificar se a camada \u00e9 ou uma camada de estilo ou de conte\u00fado, e caso seja, ele salva esse resultado. class vgg19_mod(nn.Module): def __init__(self, content_layers, style_layers): ''' Class that initialize the VGG19 model and set the intermediate style and content layers Inputs: style_layers : list of index of the style layers in the VGG19 net content_layers: list of index of the content layers in the VGG19 net ''' super(vgg19_mod, self).__init__() features = list(vgg19(pretrained = True).features) self.features = nn.ModuleList(features).eval() self.style_layers = style_layers self.content_layers = content_layers def forward(self, input): ''' Forward process that pass the input image trought the VGG 19 net and keeps the intermediates outputs Inputs: input : tensor of shape [batch, 3, 224, 224] Outputs: style_out : list of style layers outputs cotent_out : list of content layers outputs ''' style_out = [] content_out = [] for i, model in enumerate(self.features): input = model(input) if i in self.style_layers: style_out.append(input) if i in self.content_layers: content_out.append(input) return content_out, style_out","title":"Modelo"},{"location":"neural_style_transfer/#funcao-de-perda","text":"Seja \\(\\pmb x\\) o tensor da imagem que vamos treinar, \\(\\pmb p\\) \u00e9 o tensor da imagem de conte\u00fado e \\(\\pmb a\\) o tensor da imagem de estilo. A fun\u00e7\u00e3o de perda (loss) ser\u00e1 definida por dois termos, a perda de conte\u00fado e de estilo. Definimos \\(F^l\\) como a representa\u00e7\u00e3o de conte\u00fado de \\(\\pmb x\\) na camada \\(l\\) e \\(P^l\\) a representa\u00e7\u00e3o de conte\u00fado de \\(\\pmb p\\) na camada \\(l\\) . Se \\(l\\) \u00e9 uma camada que gera \\(N_l\\) filtros com um total de \\(M_l\\) elementos (largura vezes altura), ent\u00e3o \\(F_l \\in \\mathbb {R}^{N_l \\times M_l}\\) e \\(P_l \\in \\mathbb {R}^{N_l \\times M_l}\\) . A nossa perda ser\u00e1 a dist\u00e2ncia quadr\u00e1tica m\u00e9dia \\(F_l\\) e \\(P_l\\) . \\(L_{content}(\\pmb x, \\pmb p, \\pmb l) = \\dfrac{1}{N_l M_l}\\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2\\) def content_loss(pred, target): '''The content loss is the mean squared error''' return torch.pow(pred - target, 2).mean() Para a perda do estilo, em cada uma das camadas usadas para o estilo, iremos utilizar um m\u00e9todo para c\u00e1lculo de textura que computa a correla\u00e7\u00e3o, esse fator \u00e9 a matriz de Gram \\(G_l\\) , que \u00e9 definida como, o produto interno das colunas de \\(F_l\\) (a representa\u00e7\u00e3o do tensor que estamos treinando \\(\\pmb x\\) ). Agora seja \\(\\pmb a\\) o tensor de estilo e \\(A_l\\) a matriz de Gram calculada para a representa\u00e7\u00e3o de \\(\\pmb a\\) na camda \\(l\\) . A perda do estilo ser\u00e1 definida como: \\(L_{style}(\\pmb x, \\pmb a, \\pmb l) = \\dfrac{1}{N_l M_l}\\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2\\) def gram_matrice(input): ''' Compute the gram matrice of the input tensor The gram matrice of V is V'V, but we have to change the input to 2 dimensios Than we normalize by the number of elements ''' batch, channel, width, height = input.size() M = input.view(batch * channel, width * height) gram = torch.mm(M, M.t()) return gram.div(batch*width*height*channel) def style_loss(pred, gram_target): '''The style loss if the euclidian distance of the Gram matrice ''' gram_pred = gram_matrice(pred) return torch.pow(gram_pred - gram_target, 2).mean() A perda final tem ent\u00e3o a seguinte forma: $L_{total}(\\pmb x, \\pmb p, \\pmb a) = \\alpha L_{content}(\\pmb x, \\pmb p) + \\beta L_{style}(\\pmb x, \\pmb a) $ Sendo \\(\\alpha\\) e \\(\\beta\\) os par\u00e2metros que escolhemos para o treinamento dar mais prioridade para o conte\u00fado e para o estilo, reespectivamente.","title":"Fun\u00e7\u00e3o de perda"},{"location":"neural_style_transfer/#treinamento","text":"Por fim, o treinamento ocorre da seguinte forma: 1. Instanciamos o modelo. 2. Obtemos as representa\u00e7\u00f5es de conte\u00fado e de estilo das imagens de conte\u00fado e de estilo, reespectivamente. 3. Definimos uma imagem que ser\u00e1 otimizada, o valor inicial dela \u00e9 o mesmo da imagem de conte\u00fado. 4. Instanciamos o otimizador L-BFGS. 5. Para cada itera\u00e7\u00e3o da otimiza\u00e7\u00e3o: * Computamos a representa\u00e7\u00e3o de conte\u00fado e de estilo da imagem de treinamento. * Computamos a perda de conte\u00fado e de estilo das representa\u00e7\u00f5es. * Com o m\u00e9todo de backward do Pytorch, computamos os gradientes da perda. * Passamos o valor da perda e o valor do gradiente para o m\u00e9todo do L-BFGS dar um passo (step). 6. Por fim, invertemos a transforma\u00e7\u00e3o inicial de pr\u00e9-processamento, retornando a m\u00e9dia e o desvio padr\u00e3o originais. def run_style_transfer(content_path, style_path, iterations = 20, content_layers = None, style_layers = None, content_weight = 1, style_weight = 1000000, verbose = False, cuda = False, return_loss = False): ''' Training process for the neural style transfer VGG19 The method uses two images, the content and the style image, and generate a image that contain the content and the style of the reespective images The optimization is done by LBFGS Inputs: content_path : path for the content image style_path : path for the style image iterations: number of iterations to converge content_layers : list of index of VGG19 content layers style_layers : list of index of VGG19 style layers content_weight : weight of the content loss style_weight : weight of the style loss verbose : if the function will print intermediate steps cuda: if the function will process using GPU return_loss: if the function will return the loss value for each epoch Outputs: init_image : result image with content and style ''' if cuda: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if verbose: print(\"Running model with\", device) else: device = torch.device('cpu') #Create loss dict if return_loss: losses = {'style_loss': [], 'content_loss': [], 'total_loss':[]} #Setting intermediate layers if content_layers is None: content_layers = [22] if style_layers is None: style_layers = [1, 6, 11, 20, 29] #Loading the model model = vgg19_mod(content_layers, style_layers).to(device) if verbose: print('Initialized model.') #Loading the images as tensors content_img = load_image(content_path).to(device) style_img = load_image(style_path).to(device) init_img = load_image(content_path).to(device) if verbose: print('Initialized images.') #Getting the content and style outputs content_out, _ = model(content_img) _, style_out = model(style_img) if verbose: print('Calculated images outputs.') #Pre-calculating the gram matrice for the styles outputs gram_out = [gram_matrice(out) for out in style_out] #Setting optmizer optmizer = optim.LBFGS([init_img.requires_grad_()]) if verbose: print('Starting optimization.') for iter in tqdm(range(iterations)): if return_loss: _iter_style_loss = [] _iter_content_loss = [] _iter_total_loss = [] def closure(): '''Calculate ouputs, loss and gradients''' optmizer.zero_grad() init_content_out, init_style_out = model(init_img) _content_loss = 0 for l in range(len(content_out)): _content_loss += content_loss(init_content_out[l], content_out[l]) _style_loss = 0 for l in range(len(style_out)): _style_loss += style_loss(init_style_out[l], gram_out[l]) _content_loss *= content_weight _style_loss *= style_weight loss = _content_loss + _style_loss loss.backward(retain_graph=True) if iter % 100 == 0 and iter > 0: if verbose: print('Iteration %d. Model loss %.8f'%(iter, loss)) print('Content loss: %.8f | Style loss: %.8f'%(_content_loss, _style_loss)) print() #Update loss dict if return_loss: _iter_style_loss.append(_style_loss.clone().data.cpu().numpy()) _iter_content_loss.append(_content_loss.clone().data.cpu().numpy()) _iter_total_loss.append(loss.clone().data.cpu().numpy()) return _content_loss + _style_loss optmizer.step(closure) if return_loss: losses['style_loss'].append(_iter_style_loss[-1]) losses['content_loss'].append(_iter_content_loss[-1]) losses['total_loss'].append(_iter_total_loss[-1]) #Invert VGG19 input transformation invTrans = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]), transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])]) inv_img = invTrans(init_img.squeeze(0)) if return_loss: return inv_img, losses else: return inv_img","title":"Treinamento"},{"location":"neural_style_transfer/#exemplos","text":"","title":"Exemplos"},{"location":"neural_style_transfer/#gata-e-noite-estrelada-de-van-gogh","text":"Como primeiro exemplo, vamos utilizar a imagem de uma gata e a pintura Noite Estrelada do Van Gogh, no entanto, vamos variar os par\u00e2metros \\(\\alpha\\) e \\(\\beta\\) para observar o resultado na imagem final. Na realidade, iremos variar apenas a ra\u00e7\u00e3o \\(\\dfrac{\\beta}{\\alpha}\\) , pois o resultado \u00e9 o mesmo. cat_path = '/content/cat.jpg' stary_night_path = '/content/stary_night.jpg' cat_stary_dict = {} for ratio in [1e5, 1e6, 1e7]: cat_stary_dict[ratio] = run_style_transfer(cat_path, stary_night_path, verbose = False, cuda = True, content_weight = 1, style_weight = ratio) A imagem utilizada como conte\u00fado e como estilo s\u00e3o as seguintes. fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Content image') content_image = Image.open('cat.jpg') ax[0].imshow(content_image) ax[0].axis('off') ax[1].set_title(\"Style image\") style_image = Image.open('stary_night.jpg') ax[1].imshow(style_image) ax[1].axis('off') plt.show() E o resultado final \u00e9 o seguinte, para cada uma das raz\u00f5es: fig, ax = plt.subplots(nrows = 1, ncols = 3) fig.set_figwidth(20) fig.set_figheight(12) for i, ratio in enumerate([1e5, 1e6, 1e7]): img = cat_stary_dict[ratio].cpu().detach().permute(1, 2, 0).data.clamp_(0, 1).numpy() #img -= img.min() #img /= img.max() #print(img.min(), img.max()) ax[i].imshow(img) ax[i].set_title(\"Style/content weight ratio:\" + str(ratio)) ax[i].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show()","title":"Gata e Noite estrelada de Van Gogh"},{"location":"neural_style_transfer/#cachorro-e-pintura-o-grito-de-edvard-munch","text":"dog_path = '/content/little_dog.jpg' grito_path = '/content/scream.jpg' dog_grito_dict = {} for ratio in [1e5, 1e6, 1e7]: dog_grito_dict[ratio] = run_style_transfer(dog_path, grito_path, iterations = 50, verbose = False, cuda = True, content_weight = 1, style_weight = ratio) fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Content image') content_image = Image.open('little_dog.jpg') ax[0].imshow(content_image) ax[0].axis('off') ax[1].set_title(\"Style image\") style_image = Image.open('scream.jpg') ax[1].imshow(style_image) ax[1].axis('off') plt.show() fig, ax = plt.subplots(nrows = 1, ncols = 3) fig.set_figwidth(20) fig.set_figheight(12) for i, ratio in enumerate([1e5, 1e6, 1e7]): img = dog_grito_dict[ratio].cpu().detach().permute(1, 2, 0).data.clamp_(0, 1).numpy() ax[i].imshow(img) ax[i].set_title(\"Style/content weight ratio:\" + str(ratio)) ax[i].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show()","title":"Cachorro e pintura O Grito de Edvard Munch"},{"location":"neural_style_transfer/#analise-do-treinamento","text":"Vamos come\u00e7ar observando o comportamento da loss ao decorrer das itera\u00e7\u00f5es e encontrar o n\u00famero de itera\u00e7\u00f5es que otimiza o tempo de processamento e o valor da loss. Queremos obter esse valor \u00f3timo para sermos capazes de processar uma grande quantidade de imagens, como \u00e9 necess\u00e1rio ao aplicarmos o m\u00e9todo em um v\u00eddeo. cat_stary, cat_stary_loss = run_style_transfer('cat.jpg', 'stary_night.jpg', iterations = 100, cuda = True, return_loss = True) fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Model losses') ax[0].plot(list(range(len(cat_stary_loss['content_loss']))), cat_stary_loss['content_loss'], linewidth = 3, label = 'Content loss') ax[0].plot(list(range(len(cat_stary_loss['style_loss']))), cat_stary_loss['style_loss'], linewidth = 3, label = 'Style loss') ax[0].plot(list(range(len(cat_stary_loss['total_loss']))), cat_stary_loss['total_loss'], linewidth = 3, label = 'Total loss') ax[0].legend() ax[1].set_title(\"Style loss\") ax[1].set_title('Model losses') ax[1].plot(list(range(len(cat_stary_loss['content_loss']))), cat_stary_loss['content_loss'], linewidth = 3, label = 'Content loss') ax[1].plot(list(range(len(cat_stary_loss['style_loss']))), cat_stary_loss['style_loss'], linewidth = 3, label = 'Style loss') ax[1].plot(list(range(len(cat_stary_loss['total_loss']))), cat_stary_loss['total_loss'], linewidth = 3, label = 'Total loss') ax[1].set_ylim(0, 10) ax[1].set_xlim(0, 100) ax[1].legend() plt.show() Vemos que o m\u00e9todo tem uma r\u00e1pida converg\u00eancia, mantendo o mesmo valor da loss para todas as itera\u00e7\u00f5es ap\u00f3s as 30 primeiras. Dessa forma, podemos realizar o treinamento com apenas 30 itera\u00e7\u00f5es para evitar extra processamento. images = [('/content/jovem.jpg','/content/natureza_morta.jpg'), ('/content/fgv.jpg', '/content/picasso.jpg')] images_style = [] for im in images: images_style.append(run_style_transfer(im[0], im[1], iterations = 20, verbose = False, cuda = True, style_weight = ratio)) fig, ax = plt.subplots(nrows = 2, ncols = 2) ax = ax.flatten() fig.set_figwidth(20) fig.set_figheight(10) ax[0].set_title('Content image') content_image = Image.open(images[0][0]) ax[0].imshow(content_image) ax[0].axis('off') ax[1].set_title(\"Style image\") style_image = Image.open(images[0][1]) ax[1].imshow(style_image) ax[1].axis('off') ax[2].set_title('Content image') content_image = Image.open(images[1][0]) ax[2].imshow(content_image) ax[2].axis('off') ax[3].set_title(\"Style image\") style_image = Image.open(images[1][1]) ax[3].imshow(style_image) ax[3].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show() fig, ax = plt.subplots(nrows = 1, ncols = 2) fig.set_figwidth(20) fig.set_figheight(12) for i, im in enumerate(images_style): img_to_plot = im.cpu().detach().permute(1, 2, 0).data.clamp_(0, 1).numpy() ax[i].imshow(img_to_plot) ax[i].axis('off') plt.subplots_adjust(wspace=0.1, hspace=0.05) plt.show()","title":"An\u00e1lise do treinamento"},{"location":"video_style_transfer/","text":"Video Style Transfer from neural_style_transfer import style_transfer from tqdm.notebook import tqdm from PIL import Image import os import cv2 %load_ext autoreload %autoreload 2 Processamento do v\u00eddeo Carregamos o v\u00eddeo como um tensor de imagens utilizando dos m\u00e9todos do openCV. Iremos iterar por esse tensor e realizar o treinamento para cada imagem individualmente. output_path = '/content/drive/MyDrive/video_style_transfer/output_cat/' video_path = '/content/drive/MyDrive/video_style_transfer/cat_video.mp4' style_path = '/content/drive/MyDrive/video_style_transfer/stary_night.jpg' vidcap = cv2.VideoCapture(video_path) success,image = vidcap.read() count = 0 while success: image = Image.fromarray(image) model = style_transfer(image, style = style_path, output_path = output_path + 'image_'+ str(count)+'.jpg', iterations = 10, cuda = True, verbose = False) _ = model.train() success,image = vidcap.read() count += 1 Cria\u00e7\u00e3o do v\u00eddeo Agora, com a pasta de imagens, vamos utilizar da bibilioteca openCV2 para criar um arquivo mp4. def create_video_from_images(folder, out): files = os.listdir(folder) files.sort(key = lambda x : int(x[x.find('_')+1:x.find('.')])) size = (224, 224) out = cv2.VideoWriter(out+'.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 24, size) for f in tqdm(files): im = cv2.imread(folder+'/'+f) im = cv2.rotate(im, cv2.cv2.ROTATE_90_CLOCKWISE) out.write(im) create_video_from_images(output_path, '/content/drive/MyDrive/video_style_transfer/cat_style')","title":"Exemplo com v\u00eddeo"},{"location":"video_style_transfer/#video-style-transfer","text":"from neural_style_transfer import style_transfer from tqdm.notebook import tqdm from PIL import Image import os import cv2 %load_ext autoreload %autoreload 2","title":"Video Style Transfer"},{"location":"video_style_transfer/#processamento-do-video","text":"Carregamos o v\u00eddeo como um tensor de imagens utilizando dos m\u00e9todos do openCV. Iremos iterar por esse tensor e realizar o treinamento para cada imagem individualmente. output_path = '/content/drive/MyDrive/video_style_transfer/output_cat/' video_path = '/content/drive/MyDrive/video_style_transfer/cat_video.mp4' style_path = '/content/drive/MyDrive/video_style_transfer/stary_night.jpg' vidcap = cv2.VideoCapture(video_path) success,image = vidcap.read() count = 0 while success: image = Image.fromarray(image) model = style_transfer(image, style = style_path, output_path = output_path + 'image_'+ str(count)+'.jpg', iterations = 10, cuda = True, verbose = False) _ = model.train() success,image = vidcap.read() count += 1","title":"Processamento do v\u00eddeo"},{"location":"video_style_transfer/#criacao-do-video","text":"Agora, com a pasta de imagens, vamos utilizar da bibilioteca openCV2 para criar um arquivo mp4. def create_video_from_images(folder, out): files = os.listdir(folder) files.sort(key = lambda x : int(x[x.find('_')+1:x.find('.')])) size = (224, 224) out = cv2.VideoWriter(out+'.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 24, size) for f in tqdm(files): im = cv2.imread(folder+'/'+f) im = cv2.rotate(im, cv2.cv2.ROTATE_90_CLOCKWISE) out.write(im) create_video_from_images(output_path, '/content/drive/MyDrive/video_style_transfer/cat_style')","title":"Cria\u00e7\u00e3o do v\u00eddeo"}]}